{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RWgUeFn-XoEf",
        "gp8TIrvQG243",
        "Y6mne35jN5LZ",
        "_k3t-Px1Mo4D",
        "v3vtrUYGMsr0",
        "IPlDRvdHNJjE",
        "P7XfINmnDAfK",
        "lF-2boybMn8d"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/DAG-Based-Compression/blob/main/DAG_Text_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "0.  Understand connected_components(adjacency_matrix, directed=True, return_labels=True) more in-depth\n",
        "1.  Convert each token embedding in the intermediate process to a sparse matrix\n",
        "2.  Refactor the suffix tree composition algorithm to take a divide-and-conquer aka a split-and-merge approach using multithreading\n",
        "3.  Represent vectorization of text as fourth dimension, with token vector scaling corresponding to token frequencies or paths"
      ],
      "metadata": {
        "id": "ky2XwlYkVi4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages and import libraries"
      ],
      "metadata": {
        "id": "RWgUeFn-XoEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow"
      ],
      "metadata": {
        "id": "sLDhGaKUQuxE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wpoJAKxqGmT8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "from threading import Thread\n",
        "\n",
        "import math\n",
        "import random\n",
        "from queue import Queue\n",
        "import numpy as np\n",
        "import urllib.request as url\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Important functions"
      ],
      "metadata": {
        "id": "xlFwktSL-2gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "gp8TIrvQG243"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage"
      ],
      "metadata": {
        "id": "Y6mne35jN5LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DAGStore:\n",
        "  def __init__(self,\n",
        "               vertices=None,\n",
        "               edge_set=None,\n",
        "               token_index_map=None,\n",
        "               adjacency_matrix=None):\n",
        "\n",
        "      if vertices is None:\n",
        "        vertices = dict()\n",
        "      if edge_set is None:\n",
        "        edge_set = set()\n",
        "\n",
        "      self.vertices = vertices\n",
        "      self.edge_set = edge_set\n",
        "      self.token_index_map = token_index_map\n",
        "      self.reversed_token_map = None\n",
        "      # first dimension = outgoing token node\n",
        "      # second dimension = incoming token node\n",
        "      self.adjacency_matrix = adjacency_matrix"
      ],
      "metadata": {
        "id": "MBQh8GVFOCOi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SuffixNode Class\n",
        "Let:\n",
        "*    n=text length\n",
        "*    b=mean block size\n",
        "*    l=mean inter-block lexical diversity (l = n/b if every block has a unique suffix tree and l < n/b if some blocks have shared suffix tree compositions)\n",
        "\n",
        "<br>\n",
        "\n",
        "Blocked Ukkonen time complexity: O(n)\n",
        "*   If split with preprocessing (O(n) time), parallelization in a divide-and-conquer approach would offer maximum time complexity of O(b*l)\n",
        "\n",
        "\n",
        "\n",
        "pruning time complexity: O(log(b*l))\n",
        "\n",
        "tokenization time complexity: O(log(b*l))"
      ],
      "metadata": {
        "id": "_k3t-Px1Mo4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixNode:\n",
        "    def __init__(self,\n",
        "                 suffix=None,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parent=None,\n",
        "                 children=None):\n",
        "        self.suffix = suffix\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        self.parent = parent\n",
        "\n",
        "        if children is None:\n",
        "            children = set()  # Initialize children as an empty set\n",
        "        self.children = children\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.suffix\n",
        "\n",
        "\n",
        "    def split_edge(self, child, i):\n",
        "      old_suffix = child.suffix[i:]\n",
        "      new_suffix = self.suffix[i:]\n",
        "\n",
        "      # Replace the old entry for the current node with a new one for the edge split\n",
        "      split_node = SuffixNode(suffix=child.suffix[:i],\n",
        "                              frequency=child.frequency,\n",
        "                              parent=self,\n",
        "                              children=child.children)\n",
        "\n",
        "      # Remove the old child and add the new split node\n",
        "      self.children.remove(child)\n",
        "      self.children.add(split_node)\n",
        "\n",
        "      # Create a new node for the existing edge suffix\n",
        "      new_child = SuffixNode(suffix=old_suffix,\n",
        "                              frequency=1,\n",
        "                              parent=split_node,\n",
        "                              children=split_node.children)\n",
        "\n",
        "      # Transfer the children to the new split node's child\n",
        "      for grandchild in split_node.children:\n",
        "          grandchild.parent = new_child\n",
        "\n",
        "      # Create a new node for the new suffix\n",
        "      #     and move both it and the previous suffix beneath the split node\n",
        "      split_node.children = {new_child,\n",
        "                              SuffixNode(suffix=new_suffix,\n",
        "                                        frequency=1,\n",
        "                                        parent=split_node)\n",
        "      }\n",
        "\n",
        "\n",
        "    def add_suffix(self, suffix):\n",
        "\n",
        "        # Find the longest prefix match in the children\n",
        "        for child in self.children:\n",
        "\n",
        "            # Find the index of the longest common prefix\n",
        "            i = 0\n",
        "            while i < len(child.suffix) and i < len(suffix) and child.suffix[i] == suffix[i]:\n",
        "                i += 1\n",
        "\n",
        "            # If there is a common prefix\n",
        "            if i > 0:\n",
        "                # Update the frequency of the current node\n",
        "                child.frequency += 1\n",
        "\n",
        "                # If the common prefix is the entire child key, recurse into that child\n",
        "                if i == len(child.suffix):\n",
        "                    child.add_suffix(suffix[i:])\n",
        "\n",
        "                # If the common prefix is only part of the child key, split the edge\n",
        "                elif i < len(suffix):\n",
        "                    self.split_edge(child, i)\n",
        "\n",
        "                return\n",
        "\n",
        "        # No matching prefix, add the suffix as a new child\n",
        "        self.children.add(SuffixNode(suffix=suffix,\n",
        "                                     frequency=1,\n",
        "                                     parent=self))\n",
        "\n",
        "\n",
        "    def build_tree(self, text, do_blocking=True, delimeters=r\"\\n\"):\n",
        "        clauses = [text]\n",
        "        # split the text into blocks, where each block is an independent clause\n",
        "        if (do_blocking):\n",
        "          clauses = re.split(delimeters, text)\n",
        "\n",
        "        for string in clauses:\n",
        "            # loop through the string, starting with the last character\n",
        "            for i in range(0, len(string)):\n",
        "                suffix = string[len(string) - i - 1:]\n",
        "\n",
        "                # add the suffix to the tree\n",
        "                self.add_suffix(suffix)\n",
        "\n",
        "\n",
        "    def prune_tree(self, threshold=2):\n",
        "        # If the node has no children (ie it's a leaf), return\n",
        "        if not self.children:\n",
        "            # print(\"no children\")\n",
        "            return\n",
        "\n",
        "        children_to_kill = set()\n",
        "        # Recursively prune the tree\n",
        "        for child in self.children:\n",
        "\n",
        "            if isinstance(child, SuffixNode):\n",
        "                # if the child is above the threshold or it's a single character token node\n",
        "                if child.frequency >= threshold or child.parent is None:\n",
        "                    # print(\"not removed\")\n",
        "                    child.prune_tree(threshold)\n",
        "                else:\n",
        "                    # print(\"removed\")\n",
        "                    children_to_kill.add(child)\n",
        "\n",
        "        for child in children_to_kill:\n",
        "            # if the token's frequency falls below the threshold, prune it\n",
        "            self.children.remove(child)\n",
        "\n",
        "\n",
        "    # set all the suffix tree nodes' token properties\n",
        "    # return an aggregated set of all the tokens\n",
        "    def get_tokens(self, prev_token=\"\"):\n",
        "        tokens = dict()\n",
        "\n",
        "        for child in self.children:\n",
        "            if isinstance(child, SuffixNode):\n",
        "                token = prev_token + child.suffix\n",
        "\n",
        "                # set the child's token for later use\n",
        "                child.token = token\n",
        "\n",
        "                # add the accumulation of the current suffix with previous suffixes\n",
        "                #   as a new token in the token set, with the previously found frequency\n",
        "                tokens[token] = child.frequency\n",
        "                # Recursively get tokens from the child\n",
        "                tokens.update(child.get_tokens(token))\n",
        "\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "rovuxbqdXhDu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_suffix_tree(text,\n",
        "                      threshold,\n",
        "                      do_blocking=True,\n",
        "                      delimeters=r\"\\n\",\n",
        "                      tree=None):\n",
        "  # use the tree option in case a previous tree is to be pruned further\n",
        "  suffix_tree = tree\n",
        "  if suffix_tree is None:\n",
        "    suffix_tree = SuffixNode(children = { SuffixNode(suffix=unique_char) for unique_char in set(text) })\n",
        "    suffix_tree.build_tree(text,\n",
        "                          do_blocking=do_blocking,\n",
        "                          delimeters=delimeters)\n",
        "  suffix_tree.prune_tree(threshold=threshold)\n",
        "\n",
        "  tokens = suffix_tree.get_tokens()\n",
        "  alphabet = list(set(text))\n",
        "\n",
        "  return suffix_tree, tokens, { alphabet[i]: i for i in range(len(alphabet)) }"
      ],
      "metadata": {
        "id": "FHLSheWoyikQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CompositionDAGNode\n",
        "construction time complexity: O(log(|V| + |E|))"
      ],
      "metadata": {
        "id": "v3vtrUYGMsr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompositionDAGNode:\n",
        "    def __init__(self,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parents=None,\n",
        "                 children=None,\n",
        "                 dag_store=None):\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        if parents is None:\n",
        "            parents = list()\n",
        "        self.parents = parents\n",
        "\n",
        "        if children is None:\n",
        "            children = set()\n",
        "        self.children = children\n",
        "\n",
        "        if dag_store is None:\n",
        "            dag_store = DAGStore()\n",
        "        self.dag_store = dag_store\n",
        "\n",
        "    def __str__(self):\n",
        "        child_tokens = {child.token for child in self.children}\n",
        "        parent_tokens = {parent.token for parent in self.parents}\n",
        "        return f\"Token: {self.token}\\nParents: {parent_tokens}\\nChildren: {child_tokens}\"\n",
        "\n",
        "    # make an edge between the current token and a successor\n",
        "    def add_edge(self, child):\n",
        "        # Add the larger token as a child of this token\n",
        "        self.children.add(child)\n",
        "\n",
        "        # Add this token to the list of parents composing the larger token\n",
        "        child.parents.append(self)\n",
        "\n",
        "        # Add an edge to the edge list, using the current token's position\n",
        "        #   in the child token as the edge weight\n",
        "        self.dag_store.edge_set.add((self.token, child.token, len(child.parents) - 1))\n",
        "        if self.token is not None and child.token is not None:\n",
        "            self\\\n",
        "                .dag_store\\\n",
        "                .adjacency_matrix[self.dag_store.token_index_map[self.token], self.dag_store.token_index_map[child.token]] = 1\n",
        "\n",
        "\n",
        "    # since this is recursively saving smaller tokens, it's basically depth-first\n",
        "    def build_subgraph(self, all_tokens, suffix_tokenization):\n",
        "      vertices = self.dag_store.vertices\n",
        "\n",
        "      for token in suffix_tokenization:\n",
        "\n",
        "        # if the predecessor token is not in the vertex store,\n",
        "        #   recursively build a sub-graph of suffix tokens\n",
        "        if token not in vertices.keys():\n",
        "            # create a new dag node for the current token\n",
        "            #   and put it in the vertex store\n",
        "            vertices[token] = CompositionDAGNode(token=token,\n",
        "                                                 frequency=all_tokens[token],\n",
        "                                                 dag_store = self.dag_store)\n",
        "\n",
        "            # break the missing token into even smaller tokens using the largest available smaller tokens\n",
        "            curr_suffix_tokenization = tokenize(token, all_tokens, len(token) - 1)\n",
        "            # build a subgraph from the smaller tokens\n",
        "            temp_vert = vertices[token]\n",
        "            temp_vert, additional_vertices = temp_vert.build_subgraph(all_tokens, curr_suffix_tokenization)\n",
        "            vertices.update(additional_vertices)\n",
        "\n",
        "            vertices[token] = temp_vert\n",
        "\n",
        "        # base case: if the predecessor is in the vertex store\n",
        "        #   add an edge from the current node's predecessor to it\n",
        "        vertices[token].add_edge(self)\n",
        "\n",
        "      return self, vertices\n",
        "\n",
        "\n",
        "    # do breadth-first accumulation of the suffix tree into the dag\n",
        "    def suffix_tree_to_dag(self, suffix_tree):\n",
        "        all_tokens = suffix_tree.get_tokens()\n",
        "        # create a dict for mapping tokens to indices in the adjacency matrix\n",
        "        token_list = list(all_tokens.items())\n",
        "        self.dag_store.token_index_map = {token_list[i][0]: i for i in range(len(token_list))}\n",
        "        self.dag_store.reversed_token_map = {v: k for k, v in self.dag_store.token_index_map.items()}\n",
        "        # initialize the adjacency matrix in LIL format\n",
        "        #   for more efficient memory usage during composition and later modifications\n",
        "        self.dag_store.adjacency_matrix = sp.lil_matrix((len(all_tokens), len(all_tokens)))\n",
        "\n",
        "        vertices = self.dag_store.vertices\n",
        "        vertices[self.token] = self\n",
        "\n",
        "        suffix_node_queue = Queue()\n",
        "        suffix_node_queue.put(suffix_tree)\n",
        "\n",
        "        while not suffix_node_queue.empty():\n",
        "            # get the next suffix node from the queue\n",
        "            current_suffix_node = suffix_node_queue.get()\n",
        "\n",
        "            # create a dag vertex and add it to the set of vertices\n",
        "            vert = CompositionDAGNode(token=current_suffix_node.token,\n",
        "                                      frequency=current_suffix_node.frequency,\n",
        "                                      dag_store = self.dag_store)\n",
        "            vertices[vert.token] = vert\n",
        "\n",
        "            # if it's the root of the base dag or one of the top-level tokens, just add it to the vertex dict\n",
        "            if current_suffix_node.parent is None or current_suffix_node.parent.token is None:\n",
        "                vertices[current_suffix_node.token] = vert\n",
        "                # add an edge from the base graph's root to the top-level token\n",
        "                self.add_edge(vert)\n",
        "\n",
        "            # otherwise, add edges\n",
        "            else:\n",
        "                # tokenize the current token using the largest available smaller tokens\n",
        "                current_tokenization = tokenize(current_suffix_node.token,\n",
        "                                               all_tokens,\n",
        "                                               len(current_suffix_node.token) - 1)\n",
        "\n",
        "                temp_vert = vert\n",
        "                temp_vert, additional_vertices = temp_vert.build_subgraph(all_tokens, current_tokenization)\n",
        "                vertices[vert.token] = temp_vert\n",
        "                vertices.update(additional_vertices)\n",
        "\n",
        "            # add all the current node's children to the queue\n",
        "            for child in current_suffix_node.children:\n",
        "                suffix_node_queue.put(child)\n",
        "\n",
        "        # convert the LIL adjacency matrix to CSR format for more efficient modification\n",
        "        self.dag_store.adjacency_matrix = sp.csr_matrix(self.dag_store.adjacency_matrix)\n",
        "\n",
        "        self.dag_store.edge_set = {(pre, cum, pos) for pre, cum, pos in self.dag_store.edge_set if pre is not None}"
      ],
      "metadata": {
        "id": "Jnl1pI1qM2Yl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utility"
      ],
      "metadata": {
        "id": "RYBfpRS1-yAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Operational"
      ],
      "metadata": {
        "id": "IPlDRvdHNJjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, token_dict, max_token_len):\n",
        "    # filter to include only tokens shorter than the\n",
        "    available_tokens = {token: freq for token, freq in token_dict.items() if len(token) <= max_token_len}\n",
        "\n",
        "    # sort tokens by length and secondarily frequency\n",
        "    best_tokens = sorted(list(available_tokens.items()),\n",
        "                         key=lambda x: (-len(x[0]), -x[1]))\n",
        "\n",
        "    # extract tokens from the ordered list\n",
        "    tokenization = []\n",
        "    while text != \"\":\n",
        "        for token, freq in best_tokens:\n",
        "            if text.startswith(token):\n",
        "                tokenization.append(token)\n",
        "                text = text[len(token):]\n",
        "\n",
        "    return tokenization"
      ],
      "metadata": {
        "id": "ywGwIKh2-Z8x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Embedding"
      ],
      "metadata": {
        "id": "vNhEGAtBxk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distances_for_subgraph(labels, adjacency_matrix, subgraph_id):\n",
        "    \"\"\"\n",
        "    Calculate Manhattan distances for nodes in a specific subgraph and update the distance matrix.\n",
        "\n",
        "    Parameters:\n",
        "    labels (numpy array): Array of subgraph labels for each node.\n",
        "    adjacency_matrix (CSR matrix): A sparse adjacency matrix of the graph.\n",
        "    subgraph_id (int): ID of the subgraph to process.\n",
        "    \"\"\"\n",
        "    # extract a list of all the subgraph vertices in the current subgraph\n",
        "    subgraph_vertices = np.where(labels == subgraph_id)[0]\n",
        "\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    indices = []\n",
        "    values = []\n",
        "    # for each seed node\n",
        "    for i in subgraph_vertices:\n",
        "        # for each outgoing node in the subgraph\n",
        "        for x in subgraph_vertices:\n",
        "          # for each incoming node in the subgraph\n",
        "          for y in subgraph_vertices:\n",
        "              # if the entry, when double-checked, is found to be nonzero\n",
        "              #   or, if the entry represents the seed node\n",
        "              if adjacency_matrix[x, y] != 0 or i == x and x == y:\n",
        "                  # calculate inverted Manhattan distance\n",
        "                  inverse_manhattan_distance = 1 / np.maximum(np.abs(x - i) + np.abs(y - i), 1)\n",
        "\n",
        "                  # Store indices and values in lists\n",
        "                  indices.append([i, x, y])\n",
        "                  values.append(inverse_manhattan_distance)\n",
        "\n",
        "    if not indices:\n",
        "        print(\"Error, the entry for the seed node was not created in the tensor\")\n",
        "        return None\n",
        "\n",
        "    # return for recombination with the entire graph's tensor\n",
        "    return indices, values"
      ],
      "metadata": {
        "id": "01mgXVqdUbBO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def combine_sparse_tensors(tensors):\n",
        "#     if tf.shape(tensors[0].values).numpy()[0] == 0:\n",
        "#         # If tensor0 is empty, initialize it with tensor1\n",
        "#         combined_sparse = tensors[1]\n",
        "#     else:\n",
        "#         indices = []\n",
        "#         values = []\n",
        "#         dense_shape = tensors[0].dense_shape\n",
        "\n",
        "#         for tensor in tensors:\n",
        "#             indices.append(tensor.indices)\n",
        "#             values.append(tensor.values)\n",
        "\n",
        "#         print(indices)\n",
        "#         print(values)\n",
        "\n",
        "#         # Flatten and concatenate indices and values\n",
        "#         all_indices = tf.concat(indices, axis=0)\n",
        "#         all_values = tf.concat(values, axis=0)\n",
        "\n",
        "#         # Create a SparseTensor from the combined indices and values\n",
        "#         combined_sparse = tf.sparse.SparseTensor(indices=all_indices, values=all_values, dense_shape=dense_shape)\n",
        "\n",
        "#     #     # Use `tf.sparse.reduce_sum` to sum values at duplicate indices\n",
        "#     #     combined_dense = tf.sparse.to_dense(combined_sparse)\n",
        "\n",
        "#     # return tf.sparse.from_dense(combined_dense)\n",
        "#     return combined_sparse"
      ],
      "metadata": {
        "id": "-3OUnD8ioUxS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_adjacency_matrix(adjacency_matrix, low_mem=True):\n",
        "    \"\"\"\n",
        "    Vectorize the adjacency matrix by calculating Manhattan distances for each subgraph.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_matrix (CSR matrix): Adjacency matrix of the graph.\n",
        "\n",
        "    Returns:\n",
        "    sparse CSR matrix: 3D distance matrix.\n",
        "    \"\"\"\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    # Identify subgraphs and their labels\n",
        "    num_subgraphs, labels = connected_components(adjacency_matrix, directed=True, return_labels=True)\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    if low_mem:\n",
        "        # Calculate distances for each subgraph in series\n",
        "        for subgraph_id in range(num_subgraphs):\n",
        "            subgraph_distance_tensor = calculate_distances_for_subgraph(labels,\\\n",
        "                                                                        adjacency_matrix,\\\n",
        "                                                                        subgraph_id)\n",
        "\n",
        "            # if there's a subgraph distance tensor to combine the original with...\n",
        "            if subgraph_distance_tensor is not None:\n",
        "                # Recombine the partial vectorizations given by the subgraphs into\n",
        "                #   a single vectorization for the entire graph\n",
        "                # print(subgraph_distance_tensor)\n",
        "                subgraph_indices, subgraph_values = calculate_distances_for_subgraph(labels,\n",
        "                                                                                      adjacency_matrix,\n",
        "                                                                                      subgraph_id)\n",
        "                indices = indices + subgraph_indices\n",
        "                values = values + subgraph_values\n",
        "    else:\n",
        "        # Calculate distances for each subgraph in parallel\n",
        "        # Create and start threads for each subgraph\n",
        "        threads = []\n",
        "        # for subgraph_id in range(num_subgraphs):\n",
        "        #     thread = Thread(target=calculate_distances_for_subgraph,\n",
        "        #                     args=(labels,\n",
        "        #                           adjacency_matrix,\n",
        "        #                           subgraph_id))\n",
        "        #     thread.start()\n",
        "        #     threads.append(thread)\n",
        "\n",
        "        # # Wait for all threads to complete\n",
        "        # for thread in threads:\n",
        "        #     thread.join()\n",
        "        pass\n",
        "\n",
        "\n",
        "    print(indices, values)\n",
        "\n",
        "    # Return a sparse tensor for storing the tokens' distance tensors/embeddings\n",
        "    return tf.sparse.SparseTensor(indices=indices,\n",
        "                                  values=values,\n",
        "                                  dense_shape=[n, n, n])"
      ],
      "metadata": {
        "id": "y7oPMU_HPRds"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_csr(tensor):\n",
        "  # Convert SparseTensor to COO format\n",
        "  indices_np = np.array(tensor.indices.numpy())\n",
        "  values_np = np.array(tensor.values.numpy())\n",
        "  shape_np = tensor.dense_shape.numpy()\n",
        "\n",
        "  print(shape_np)\n",
        "\n",
        "  # Ensure indices_np contains pairs of row and column indices\n",
        "  if len(indices_np.shape) == 1:\n",
        "      # Reshape indices_np if it's one-dimensional\n",
        "      indices_np = indices_np.reshape(-1, 2)\n",
        "\n",
        "  coo = sp.coo_matrix((values_np, (indices_np[:, 0], indices_np[:, 1])), shape=(tensor.dense_shape[0], tensor.dense_shape[1]))\n",
        "\n",
        "  # Convert COO matrix to CSR format and retrun the reformatted tensor\n",
        "  return coo.tocsr()"
      ],
      "metadata": {
        "id": "Vhf2ZOp3q9Ai"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(adjacency_matrix, reversed_token_map, tokenizations):\n",
        "  n = len(tokenizations)\n",
        "  token_tensor = vectorize_adjacency_matrix(adjacency_matrix)\n",
        "  # print(str(token_tensor)[:50])\n",
        "  # print(composition_dag.dag_store.reversed_token_map)\n",
        "  # create a dictionary of all the tokens and their respective tensor embedding slices\n",
        "  # print(reversed_token_map)\n",
        "  token_vector_mappings = {reversed_token_map[i]: tf.sparse.slice(token_tensor,\n",
        "                                                                  start=[i, 0, 0],\n",
        "                                                                  size=[1, token_tensor.shape[1], token_tensor.shape[2]]) for i in range(n)}\n",
        "\n",
        "  print(token_vector_mappings)\n",
        "\n",
        "  for key, token_set in tokenizations.items():\n",
        "    for token in token_set.keys():\n",
        "      print(token)\n",
        "      tok_vect_tensor = token_vector_mappings[token]\n",
        "      print(tok_vect_tensor)\n",
        "\n",
        "      token_vector_mappings[token] = tensor_to_csr(token_vector_mappings[token])\n",
        "      print(token_vector_mappings[token])\n",
        "\n",
        "  return token_vector_mappings"
      ],
      "metadata": {
        "id": "V2bv4PySphAC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting and Printing"
      ],
      "metadata": {
        "id": "P7XfINmnDAfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(tree, indent = 0):\n",
        "    # Iterate over the child.suffixes (features) in the tree\n",
        "    for child in tree.children:\n",
        "        print(' ' * indent + str(child.suffix) + \": \" + str(child.frequency))\n",
        "        # If the child is a SuffixNode, recursively print the subtree\n",
        "        if isinstance(child, SuffixNode):\n",
        "            print_tree(child, indent + 4)\n",
        "        else:\n",
        "            print(' ' * (indent + 4) + \"\\\"\" + str(child.suffix) + \"\\\": \" + str(child.frequency))"
      ],
      "metadata": {
        "id": "obhJEZC3d-rO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(embeddings, max_plots):\n",
        "  i = 0\n",
        "  for token, vector in embeddings.items():\n",
        "    if (i < max_plots):\n",
        "        print(\"Embedding for'\" + token + \"':\")\n",
        "\n",
        "        # Create the heatmap\n",
        "        sns.heatmap(vector, annot=False, cmap='viridis')\n",
        "\n",
        "        # Display the heatmap\n",
        "        plt.show()\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "d7I3OmAALAiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dag(dag_store, A=None, edge_set=None, scaling=50, edge_width=1, k=2):\n",
        "\n",
        "    if A is None:\n",
        "        # Initialize a directed graph\n",
        "        G = nx.DiGraph()\n",
        "        # Add edges to the graph\n",
        "        for parent, child, position in edge_set:\n",
        "            G.add_edge(parent, child)#, weight=position)\n",
        "\n",
        "    else:\n",
        "        # Create a directed graph from the adjacency matrix\n",
        "        G = nx.from_numpy_array(A, create_using=nx.DiGraph)\n",
        "\n",
        "        # Relabel the token nodes\n",
        "        G = nx.relabel_nodes(G, dag_store.reversed_token_map)\n",
        "\n",
        "        # # Draw the graph without edge labels\n",
        "        # # Convert nodes to strings before calculating length\n",
        "        # nx.draw(G, with_labels=True, node_size=[scaling * len(str(node)) for node in G.nodes()],\n",
        "        #         width=edge_width, font_size=8)\n",
        "\n",
        "\n",
        "    # Calculate figure size based on the number of nodes\n",
        "    num_nodes = len(G.nodes)\n",
        "    num_edges = len(G.edges)\n",
        "    graph_size = (num_nodes) + (2 * num_edges)\n",
        "\n",
        "    figsize = graph_size * scaling/300\n",
        "    font_size = 2 + math.sqrt(scaling)/5\n",
        "\n",
        "    # Position nodes using the spring layout\n",
        "    pos = nx.spring_layout(G, seed=42, k=k/num_nodes)\n",
        "\n",
        "    # Calculate node sizes based on the length of the token text\n",
        "    node_sizes = [scaling * len(node) for node in G.nodes()]\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize), dpi=300)\n",
        "\n",
        "    # Draw nodes with sizes proportional to the length of their text\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes)\n",
        "\n",
        "    # Draw edges with widths based on edge weights\n",
        "    nx.draw_networkx_edges(G,\n",
        "                          pos,\n",
        "                          edgelist=edge_set,\n",
        "                          width=edge_width,\n",
        "                          arrowstyle='-|>',\n",
        "                          connectionstyle=\"arc3,rad=0.2\")\n",
        "\n",
        "    # Draw node labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=font_size, font_family=\"sans-serif\")\n",
        "\n",
        "    # # Draw edge weight labels\n",
        "    # edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "    # nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "    # Customize and show plot\n",
        "    ax = plt.gca()\n",
        "    ax.margins(0.08)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sHXlkuoDNxEd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "8Z3kELrESdur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables and functions"
      ],
      "metadata": {
        "id": "5tV0r9uAMCLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing function"
      ],
      "metadata": {
        "id": "lF-2boybMn8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(text,\n",
        "             min_freq,\n",
        "             blocking,\n",
        "             delimeters=r\"\\n\",\n",
        "             tree=None):\n",
        "  suffix_tree, tokenizations[(text, min_freq)], alphabet = build_suffix_tree(text,\n",
        "                                                                 min_freq,\n",
        "                                                                 do_blocking=blocking,\n",
        "                                                                 delimeters=delimeters,\n",
        "                                                                 tree=tree)\n",
        "  # print(\"modified suffix tree composed in \", time.time() - start_time, \" seconds.\")\n",
        "  # print_tree(suffix_tree)\n",
        "\n",
        "  composition_dag = CompositionDAGNode()\n",
        "  start_time = time.time()\n",
        "  composition_dag.suffix_tree_to_dag(suffix_tree)\n",
        "  end_time = time.time() - start_time\n",
        "  # print(\"dag composed in \", end_time, \" seconds.\")\n",
        "\n",
        "  if (min_freq % int(max(freq_range)/num_graphs_to_plot) == 0):\n",
        "      plot_dag(composition_dag.dag_store,\n",
        "               A=composition_dag.dag_store.adjacency_matrix,\n",
        "               k=4,\n",
        "               scaling=25)\n",
        "\n",
        "  # get tensor embeddings for all vertices\n",
        "  token_vector_mappings = vectorize(composition_dag.dag_store.adjacency_matrix,\n",
        "                                    composition_dag.dag_store.reversed_token_map,\n",
        "                                    tokenizations)\n",
        "\n",
        "  # return {(pre, cum, pos+1) for pre, cum, pos in composition_dag.dag_store.edge_set if pre is not None}\n",
        "  return end_time, suffix_tree, token_vector_mappings"
      ],
      "metadata": {
        "id": "w3R0E36ZyPZ4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Variables"
      ],
      "metadata": {
        "id": "t5jAIsElMq40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freq_range = range(30, 1010, 10)\n",
        "freq_range = range(5, 25, 5)\n",
        "folds = 1\n",
        "num_graphs_to_plot = 1\n",
        "max_vector_plots = 2\n",
        "tokenizations = dict()\n",
        "test_results = {\n",
        "    \"min frequency\": [],\n",
        "    \"test number\": [],\n",
        "    \"blocking?\": [],\n",
        "    \"mean time\": [],\n",
        "}"
      ],
      "metadata": {
        "id": "Z7TfB_s2fVeW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = \"https://courses.cs.washington.edu/courses/cse163/20wi/files/lectures/L04/bee-movie.txt\"\n",
        "with url.urlopen(test_url) as f:\n",
        "    text = f.read().decode('utf-8')\n",
        "# previously 454:500\n",
        "# text = text[0:2000]\n",
        "text = \"abbabba\"\n",
        "\n",
        "tests = [text]\n",
        "\n",
        "print(text[:50])"
      ],
      "metadata": {
        "id": "13ofz24yHJ9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2b224c-86cd-4c20-a899-508107b01f00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abbabba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run tests"
      ],
      "metadata": {
        "id": "ytUGfTCFMLsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# blocking or non-blocking\n",
        "for j in range(2):\n",
        "  print(\"blocking = \", bool(j))\n",
        "  prev_trees = dict()\n",
        "\n",
        "  for min_freq in freq_range:\n",
        "    print(\"minimum frequency: \", min_freq)\n",
        "\n",
        "    for i in range(len(tests)):\n",
        "      # print(\"test \", i)\n",
        "      # if there's no previous tree stored for this test\n",
        "      if i not in prev_trees.keys():\n",
        "        prev_trees[i] = None\n",
        "\n",
        "      mean_times = [0, 0]\n",
        "      for fold in range(folds):\n",
        "        new_time=0\n",
        "        new_time, prev_trees[i], token_vector_mappings = run_test(text=tests[i],\n",
        "                                                          min_freq=min_freq,\n",
        "                                                          blocking=bool(j),\n",
        "                                                          delimeters=r\"\\n\",     #r\"\\n\\n|.\\n|\\)\\n|:|\\.\\.\\.\",\n",
        "                                                          tree=prev_trees[i])\n",
        "        mean_times[j] += new_time\n",
        "\n",
        "        plot_embeddings(token_vector_mappings, max_vector_plots)\n",
        "\n",
        "      test_results[\"min frequency\"].append(min_freq)\n",
        "      test_results[\"test number\"].append(i)\n",
        "      test_results[\"blocking?\"].append(j)\n",
        "      test_results[\"mean time\"].append(mean_times[j]/folds)\n",
        "\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "imsuKapecmdG",
        "outputId": "ce8b04ac-6040-451f-d1c7-17e67e00c5a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocking =  False\n",
            "minimum frequency:  5\n",
            "[[0, 0, 0], [1, 1, 1]] [1.0, 1.0]\n",
            "{'a': SparseTensor(indices=tf.Tensor([[0 0 0]], shape=(1, 3), dtype=int64), values=tf.Tensor([1.], shape=(1,), dtype=float64), dense_shape=tf.Tensor([1 2 2], shape=(3,), dtype=int64))}\n",
            "a\n",
            "SparseTensor(indices=tf.Tensor([[0 0 0]], shape=(1, 3), dtype=int64), values=tf.Tensor([1.], shape=(1,), dtype=float64), dense_shape=tf.Tensor([1 2 2], shape=(3,), dtype=int64))\n",
            "[1 2 2]\n",
            "  (0, 0)\t1.0\n",
            "b\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'b'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-98aaa740a885>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnew_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         new_time, prev_trees[i], token_vector_mappings = run_test(text=tests[i],\n\u001b[0m\u001b[1;32m     19\u001b[0m                                                           \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                           \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c608fe93a690>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(text, min_freq, blocking, delimeters, tree)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# get tensor embeddings for all vertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   token_vector_mappings = vectorize(composition_dag.dag_store.adjacency_matrix,\n\u001b[0m\u001b[1;32m     28\u001b[0m                                     \u001b[0mcomposition_dag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdag_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreversed_token_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                     tokenizations)\n",
            "\u001b[0;32m<ipython-input-35-147655a90bd3>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(adjacency_matrix, reversed_token_map, tokenizations)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mtok_vect_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_vector_mappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_vect_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'b'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests_df = pd.DataFrame.from_dict(test_results)\n",
        "tests_df.head()"
      ],
      "metadata": {
        "id": "amIeLCRJFLtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noblock = tests_df[tests_df[\"blocking?\"] == 0]\n",
        "block = tests_df[tests_df[\"blocking?\"] == 1]\n",
        "plt.plot(noblock[\"min frequency\"],\n",
        "          noblock[\"mean time\"],\n",
        "          \"r\",\n",
        "          block[\"min frequency\"],\n",
        "          block[\"mean time\"],\n",
        "          \"g\"\n",
        ")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.xlabel('Min Frequency (log scale)')\n",
        "plt.ylabel('Mean Time (log scale)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJEfM6CFHgcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMZ5smFRIQdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}